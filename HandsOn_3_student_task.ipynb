{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiTUfcgMOs6q"
      },
      "source": [
        "Notebooks created by Oliver Hausdörfer, Jonathan Külz, Hannah Markgraf.\n",
        "\n",
        "**How to use this notebook:** You can upload this .ipynb-file to google colab for editing and running (go to [colab.research.google.com](colab.research.google.com) -> File -> Open Notebook -> Upload).\n",
        "\n",
        "**How to submit this notebook:** After finishing the HandsOn on google colab, share the uploaded notebook (click share -> Anyone with link) and send the link with the <font color='red'>completed notebook and cell outputs</font> to the course supervisor oliver.hausdoerfer@tum.de. Include your name and student ID in this e-mail.\n",
        "\n",
        "<font color='red'>Additionally:</font> this course is offered for the first time. Please provide us feedback regarding the programming exercises. If you want, you can use an anonymized e-mail to send us the feedback. If you use your TUM-Email the feedback will not be anonymous. Your feedback will not influence your evaluation results. Providing feedback is not required, but optional.\n",
        "\n",
        "**All tasks that need to be completed by you are marked with:** \"👉 TODO: ...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6tjI2tHQ8j"
      },
      "source": [
        "# HandsOn 3: Q-Learning 🤖\n",
        "\n",
        "At the end of the notebook, you will:\n",
        "\n",
        "- Be able to implement a simple a Q-Learning agent.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ONOODsyrMU"
      },
      "source": [
        "## Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfb0ZRyYeffa"
      },
      "source": [
        "**Goal: learning the Q-Function.**\n",
        "\n",
        "- The **Q-Function** is the state-action-value function, i.e., it holds the values for taking each possible action in each state. For real world continuous problems, the Q-Function cannot be represented by a table. Instead, a function approximator, i.e., neural network, is chosen. The original Deep Q-Learning (DQN) approach was proposed in [this](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) paper.\n",
        "\n",
        "- In this notebook, we will stick to a small discrete toy problem, the [Frozen Lake Grid World](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) in which the Q-Function can be represented by a table.\n",
        "\n",
        "- At the beginning, the Q-Table (or Neural Network) is initialized with arbitrary values for each state-action pair. As we explore the environment and update our Q-Table, i.e., during learning, it will give us better and better approximations of the true Q-Value.\n",
        "\n",
        "- In our case, when the training is done, we have an optimal Q-Table. Consequently, we also have the optimal policy, since we can query the best action for every state.\n",
        "\n",
        "The following is one Q-Learning algorithm taken from the Sutton & Barto book.\n",
        "\n",
        "\n",
        "![q_learning_algorithm](https://raw.githubusercontent.com/OliEfr/HandsOnDRL_LabCourse/main/q_learning_algo_2.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEtx8Y8MqKfH"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYQm2vpwg-bV"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAvihuHdy9tw"
      },
      "source": [
        "👉TODO: As always before starting have a look at the [FrozenLake environment](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws4pQPrhhota"
      },
      "source": [
        "We are going to use the environment with the following configuration:\n",
        "- `map_name=\"4x4\"`: a 4x4 grid version\n",
        "- `is_slippery=False`: The agent always moves in the intended direction (deterministic).\n",
        "\n",
        "The goal is to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzJnb8O3y8up"
      },
      "outputs": [],
      "source": [
        "# 👉TODO: Create the FrozenLake-v1 environment using 4x4 map and non-slippery version (1 LOC)\n",
        "env = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "We see with `Observation Space Shape Discrete(16)` that the observation is an integer representing the **agent’s current position as current_row * ncols + current_col (where both the row and col start at 0)**.\n",
        "\n",
        "For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. **For example, the 4x4 map has 16 possible observations.**\n",
        "\n",
        "\n",
        "For instance, this is what state = 0 looks like:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available 🎮:\n",
        "- 0: GO LEFT\n",
        "- 1: GO DOWN\n",
        "- 2: GO RIGHT\n",
        "- 3: GO UP\n",
        "\n",
        "Reward function:\n",
        "- Reach goal: +1\n",
        "- Reach hole: 0\n",
        "- Reach frozen: 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pFhWblk3Awr"
      },
      "source": [
        "## Create and Initialize the Q-table (Step 1)\n",
        "\n",
        "\n",
        "\n",
        "![q_learning_algorithm](https://raw.githubusercontent.com/OliEfr/HandsOnDRL_LabCourse/main/q_learning_algo_2.png)\n",
        "\n",
        "To init our Q-Table, we need to know how many rows (states) and columns (actions) to use. We already know their values from before, but we'll want to obtain them programmatically so that our algorithm generalizes for different environments. Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3ZCdluj3k0l"
      },
      "outputs": [],
      "source": [
        "# 👉 TODO: Complete this code cell\n",
        "# States are often used as a synonym for observations, although they are technically not the same. See OpenAI Spinning Up documentation for details.\n",
        "state_space = # number of states\n",
        "print(\"There are \", state_space, \" possible state.\")\n",
        "\n",
        "action_space = # number of observations\n",
        "print(\"There are \", action_space, \" possible actions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCddoOXM3UQH"
      },
      "outputs": [],
      "source": [
        "# Create the Q-table of size (state_space, action_space) and initialized each values at 0 using np.zeros.\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YfvrqRt3jdR"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atll4Z774gri"
      },
      "source": [
        "## Define the greedy policy\n",
        "\n",
        "Since we are implementing an off-policy algorithm, we have two policies. This means we're using a **different policy for acting and updating the value function**.\n",
        "\n",
        "- Epsilon-greedy policy (acting policy)\n",
        "- Greedy-policy (updating policy)\n",
        "\n",
        "The greedy policy will also be the final policy we'll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table.\n",
        "\n",
        "The following slide is taking from the huggingface tutorial of DRL. Note that on- an off-policy DRL algorithms have their distinct advantages and disadvantages. More details regarding On-Policy (SARSA) and Off-Policy (Q-Learning) can be found [here](https://stackoverflow.com/questions/6848828/what-is-the-difference-between-q-learning-and-sarsa).\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3SCLmLX5bWG"
      },
      "outputs": [],
      "source": [
        "# 👉TODO: Implement the greedy policy by selecting the action with the highest state-action value given a state (requires 1 LOC)\n",
        "# and then return that action (requires another 1 LOC)\n",
        "def greedy_policy(Qtable, state):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flILKhBU3yZ7"
      },
      "source": [
        "## Define the epsilon-greedy policy\n",
        "\n",
        "Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.\n",
        "\n",
        "The idea with epsilon-greedy:\n",
        "\n",
        "- With *probability 1 - ɛ* : we do **exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n",
        "\n",
        "- With *probability ɛ*: we do **exploration** (trying a random action).\n",
        "\n",
        "As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bj7x3in3_Pq"
      },
      "outputs": [],
      "source": [
        "# 👉 TODO: complete the following function\n",
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num = random.uniform(0,1)\n",
        "  # if random_num > greater than epsilon: exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # (=greedy) (1 LOC)\n",
        "    action = \n",
        "  # else: exploration\n",
        "  else:\n",
        "    # Take a random action\n",
        "    action = \n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW80DealcRtu"
      },
      "source": [
        "## Define the hyperparameters ⚙️\n",
        "\n",
        "The exploration related hyperparamters are some of the most important ones.\n",
        "\n",
        "- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n",
        "- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1tWn0tycWZ1"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10000  # Total training episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "eval_seed = []               # The evaluation seed of the environment\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05            # Minimum exploration probability\n",
        "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDb7Tdx8atfL"
      },
      "source": [
        "## Create the training loop method\n",
        "\n",
        "The training loop goes like this:\n",
        "\n",
        "```\n",
        "Loop for each episode:\n",
        "  Reduce epsilon\n",
        "  Reset environment\n",
        "    For step in max_steps:    \n",
        "      Choose the action A_t using epsilon greedy policy\n",
        "      Take the action A and observe the outcome state (s') and reward (r)\n",
        "      Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      If done, finish the episode\n",
        "      Our next state is the new state\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9yKbiYpVqDC"
      },
      "source": [
        "The most important thing to note here is the [Bellman Equation](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html), which is used to iteratively optimize the Q-Function. The Bellman Equation is a central element of many DRL algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paOynXy3aoJW"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # Reduce epsilon\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # 👉TODO: Choose the action At using epsilon greedy policy (1 LOC)\n",
        "      action = \n",
        "\n",
        "      # 👉TODO: Take action At and observe Rt+1 and St+1 (1 LOC)\n",
        "      new_state, reward, terminated, truncated, info = \n",
        "\n",
        "      # 👉TODO: Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)] (1 LOC)\n",
        "      Qtable[state][action] = \n",
        "\n",
        "      # If terminated or truncated finish the episode\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwKQ4tUdhGI"
      },
      "source": [
        "## Train the Q-Learning agent 🏃"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPBxfjJdTCOH"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeEhUCrc30L"
      },
      "source": [
        "Inspect the Q-Table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmfchsTITw4q"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChiVuZxqh49G"
      },
      "source": [
        "👉TODO: Will the learned policy lead us to the desired goal? Explain your decision _briefly_ with the help of one example from the Q-Table. Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMXd4VReWKf5"
      },
      "source": [
        "Evaluate the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNl0_JO2cbkm"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param max_steps: Maximum number of steps per episode\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param Q: The Q-table\n",
        "  :param seed: The evaluation seed array (for taxi-v3)\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    if seed:\n",
        "      state, info = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state, info = env.reset()\n",
        "    step = 0\n",
        "    truncated = False\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # Take the action that has the maximum expected future reward given that state\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJqjaoAnxUo"
      },
      "source": [
        "Usually, you should have a mean reward of 1.0. The environment is relatively easy since the state space is small (16). What you can try to do is [to replace it with the slippery version](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAgB7s0HEFMm"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfkgG95mWZ5V"
      },
      "source": [
        "You successfully implemented a Reinforcement Learning Agent using Q-Learning! The reason why such a simple algorithm can not be used for most real world problems is that it only applies for discrete environments where all Q-Values can be listed. Due to the curse of dimensionality it is not possible to store all Q-Values in systems with many states and actions (and in the continuous case anyway).\n",
        "\n",
        "That is where neural networks and more advanced algorithms come into play. They tackle the problem of scaling DRL to continuous systems with larger spaces. Although the foundations of many of these algorithms are the same, they implement different methodologies to improve training routines and convergence. For instance, PPO limits the update step of the policy (_proximal_ policy optimization) and a experience replay buffer decorrelates samples for training (batch training as in DQN).\n",
        "\n",
        "For problems in robotics you will usually use some of the more advanced algorithms. One example of such a task is handling a Rubik's Cube with a robot hand in [this paper](https://arxiv.org/pdf/1910.07113.pdf). 👉LAST TODO: _Briefly_ answer the following questions about that paper:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWfJbaeoaFNr"
      },
      "source": [
        "- What is domain randomization used for? Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlo33GmYaPAx"
      },
      "source": [
        "- Which DRL algorithm was used? Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmyW7K7YazBY"
      },
      "source": [
        "-  Do they use any type of behavioral cloning, if so, which one? Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGKUqO-aa3zI"
      },
      "source": [
        "- What is the dimensionality of the observation space? Name three different observations chosen for the policy net! Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHKFCY9JbQ1s"
      },
      "source": [
        "# HandsOn 3: Finished 🤖"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsydSn3CbW2w"
      },
      "source": [
        "Here are some more resources and information to play around with\n",
        "\n",
        "Guides on how to do DRL projects (recommended to check out before starting your practical!):\n",
        "\n",
        "\n",
        "*   https://github.com/williamFalcon/DeepRLHacks\n",
        "*   https://rockt.github.io/2018/08/29/msc-advice\n",
        "*   https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
        "\n",
        "Other resources\n",
        "*   https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html (recommended)\n",
        "*   https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n",
        "*   https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
        "*   http://incompleteideas.net/book/the-book-2nd.html\n",
        "*   DRL tutorials by hugging face: https://huggingface.co/learn/deep-rl-course/unit0/introduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nva66l3hJ6ka"
      },
      "source": [
        "**How to submit this notebook:** After finishing the HandsOn on google colab, share the uploaded notebook (click share -> Anyone with link) and send the link with the <font color='red'>completed notebook and cell outputs</font> to the course supervisor oliver.hausdoerfer@tum.de. Include your name and student ID in this e-mail.\n",
        "\n",
        "<font color='red'>Additionally:</font> this is a first years course. Please provide us feedback regarding the programming exercises. If you want, you can use an anonymized e-mail to send us the feedback. If you use your TUM-Email the feedback will not be anonymous. Your feedback will not influence your evaluation results. Providing feedback is not required, but optional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJls6HoFilU7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "67OdoKL63eDD",
        "B2_-8b8z5k54",
        "8R5ej1fS4P2V",
        "Pnpk2ePoem3r"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
