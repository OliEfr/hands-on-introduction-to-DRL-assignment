{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTuK7eeXvwQx"
      },
      "source": [
        "Notebooks created by Oliver HausdÃ¶rfer, Jonathan KÃ¼lz, Hannah Markgraf.\n",
        "\n",
        "**How to use this notebook:** You can upload this .ipynb-file to google colab for editing and running (go to [colab.research.google.com](colab.research.google.com) -> File -> Open Notebook -> Upload).\n",
        "\n",
        "**How to submit this notebook:** After finishing the HandsOn on google colab, share the uploaded notebook (click share -> Anyone with link) and send the link with the <font color='red'>completed notebook and cell outputs</font> to the course supervisor oliver.hausdoerfer@tum.de. Include your name and student ID in this e-mail.\n",
        "\n",
        "**All tasks that need to be completed by you are marked with:** \"ðŸ‘‰ TODO: ...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "# HandsOn 2: Your second Deep Reinforcement Learning Agent ðŸ¤–\n",
        "\n",
        "In this notebook, you'll train your second Deep Reinforcement Learning agent - a [CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) that will learn to balance an inverted pendulum. For the project, we again use [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) and [Gymnasium](https://gymnasium.farama.org/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6tjI2tHQ8j"
      },
      "source": [
        "## Objectives of this Hands-On\n",
        "\n",
        "At the end of the notebook, you will:\n",
        "\n",
        "- Be able to use [OpenAI Gymnasium](https://gymnasium.farama.org/index.html), the environment library.\n",
        "- Be able to use [Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/) (SB3), the deep reinforcement learning library.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqzznTzhNfAC"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type > Hardware Accelerator > GPU`. **Important**: You need to reconnect your colab notebook afterward!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "!apt-get update && apt-get install ffmpeg freeglut3-dev xvfb  # For visualization. If you are working on your own machine, run these in your terminal as sudo\n",
        "!pip install -q \"stable-baselines3\"\n",
        "!pip install -q \"swig\"\n",
        "!pip install -q \"gymnasium[box2d]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c8pxXT8mI9G"
      },
      "outputs": [],
      "source": [
        "import stable_baselines3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import base64\n",
        "from pathlib import Path\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# for local usage\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "print(f\"{torch.__version__=}\")\n",
        "print(f\"{stable_baselines3.__version__=}\")\n",
        "print(f\"{gym.__version__=}\")\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Did you activate the GPU? It doesn't seem to be available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRqRuRUl8CsB"
      },
      "source": [
        "### Reinforcement Learning Agent ðŸ¤–\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ8VA-GBx9Yh"
      },
      "source": [
        "ðŸ‘‰ TODO: Familiarize yourself with the CartPole-v1 environment. Then, fillout the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5vN47kmx-gW"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()\n",
        "\n",
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", '''ðŸ‘‰ TODO: insert line of code to print action space shape''')\n",
        "print(\"Action Space Sample\", '''ðŸ‘‰ TODO: insert line of code to randomly sample from action space''')\n",
        "\n",
        "\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", '''ðŸ‘‰ TODO: insert line of code to print observation space shape''')\n",
        "print(\"Sample observation\", '''ðŸ‘‰ TODO: insert line of code to get a random observation''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hj77AbHylpY"
      },
      "source": [
        "Similar to our last notebook we first create the agent-environment-interaction loop by sampling random actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7vOFlpA_ONz"
      },
      "outputs": [],
      "source": [
        "# Reset the environment\n",
        "observation, info = env.reset()\n",
        "\n",
        "# Move 20 timesteps\n",
        "for _ in range(20):\n",
        "  # Take a random action\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "\n",
        "  # Exectue this action in the environment and get information\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  if terminated or truncated:\n",
        "      # Reset the environment\n",
        "      print(\"Environment is reset\")\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFD9RAFjG8aq"
      },
      "source": [
        "**Hint: Vectorized Environments**\n",
        "\n",
        "In practise when you implement your own DRL frameworks, you should aim at using [Vectorized environments](https://stable-baselines3.readthedocs.io/en/master/common/env_util.html#stable_baselines3.common.env_util.make_vec_env). They are a method for stacking multiple independent environments, which diversifies experience collection and potentially improves training speed through parallelization. SB3 implements [vectorized environments](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#) ðŸ‘‰ TODO: Make sure to understand the different types of vec envs provided by SB3 [SubprocVecEnv](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#stable_baselines3.common.vec_env.SubprocVecEnv) and [DummyVecEnv](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#dummyvecenv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99hqQ_etEy1N"
      },
      "outputs": [],
      "source": [
        "# SB3 provides the convinience function to create a vectorized environment\n",
        "env = make_vec_env('CartPole-v1', n_envs=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR0DPQQjsE0E"
      },
      "outputs": [],
      "source": [
        "# ðŸ‘‰ TODO: briefly explain what type of vectorized environment (Subproc or Dummy) you would use to speed up simulation on a multi-core cpu machine. (You don't need to go into very technical details.)\n",
        "print(\"I would use ..., because ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV4yiUM_9_Ka"
      },
      "source": [
        "Now we solve (=train) our agent using [PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D). PPO is an actor-critic algorithm and one of the SOTA Deep Reinforcement Learning algorithms. Actor-critic means that it learns a policy function and a value function simultaneously. The policy learns which action to take in which state (s->a) and the value function tells the value of each state (s->value). The goal is for the agent to explore the full state space. PPO implements some additional tweaks to make the training more stable, such as clipping the loss function.\n",
        "\n",
        "We already created our environment above. Now we need to setup the DRL model we want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxI6hT1GE4-A"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "# ðŸ‘‰ TODO: Initialize the PPO agent. Use the following parameters:\n",
        "    # policy = 'MlpPolicy',\n",
        "    # env = env,\n",
        "    # n_steps = 1024,\n",
        "    # batch_size = 64,\n",
        "    # n_epochs = 4,\n",
        "    # gamma = 0.99,\n",
        "    # gae_lambda = 0.98,\n",
        "    # ent_coef = 0.01\n",
        "# Documentation: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
        "# Additionally, define a custom policy and value network using 2 hidden layers, 32 neurons and ReLU as activation.\n",
        "# The policy and value network should be independent networks.\n",
        "# Documentation: https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
        "# Requires ~10 LOC\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oHaE24S1VYw"
      },
      "source": [
        "We inspect the network layouts to see if it is initialized correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZltDG2d41avN"
      },
      "outputs": [],
      "source": [
        "model.policy.mlp_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJJk88yoBUi"
      },
      "source": [
        "Lastly, we simply train the agent.\n",
        "\n",
        "ðŸ‘‰ TODO: try different number of agent-environment interactions for learning (```total_timesteps```) in the range of 10_000 to 200_000 and see how the achieved reward changes using the ```evaluate_policy``` function given below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKnYkNiVp89p"
      },
      "outputs": [],
      "source": [
        "model.learn(total_timesteps=10_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY_HuedOoISR"
      },
      "source": [
        "---\n",
        "\n",
        "**Evaluating the agent**\n",
        "\n",
        "Stable-Baselines3 provides a method to evaluate the agent: `evaluate_policy`. [Under the hood](https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/evaluation.html#evaluate_policy) this method simply implements an environment-interaction-loop. When you evaluate your agent, you should not use your training environment but create an evaluation environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRpno0glsADy"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "eval_env = gym.make(\"CartPole-v1\",)\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTqwBQDxxZ8j"
      },
      "source": [
        "Finally, we prepare a video of the trained agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnElRfHkxX6N"
      },
      "outputs": [],
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "\n",
        "\n",
        "def show_videos(video_path=\"\", prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "    :param video_path: (str) Path to the folder containing videos\n",
        "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
        "    \"\"\"\n",
        "    :param env_id: (str)\n",
        "    :param model: (RL model)\n",
        "    :param video_length: (int)\n",
        "    :param prefix: (str)\n",
        "    :param video_folder: (str)\n",
        "    \"\"\"\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")])\n",
        "    # Start the video at step=0 and record 500 steps\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix,\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    # Close the video recorder\n",
        "    eval_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33kXppGvxh_c"
      },
      "outputs": [],
      "source": [
        "record_video(\"CartPole-v1\", model, video_length=500, prefix=\"ppo-cartpole\")\n",
        "show_videos(\"videos\", prefix=\"ppo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbsOCXr0gku-"
      },
      "source": [
        "**Important note**\n",
        "\n",
        "Usually crafting a reward function is a central task of solving DRL problems. For the gym environments we use, reward functions are already provided and we don't need to worry about them. For your own projects, the crafting of rewards might be critical. Before creating a reward function it makes sense to look at similar problems in the literature and try to copy their rewards.\n",
        "\n",
        "ðŸ‘‰ TODO: Look at [this](https://www.nature.com/articles/s41598-023-38259-7) paper and how they design a reward function for a quadruped to get an idea of the complexity of the reward signal for a more complicated problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL-0TmrCN6xl"
      },
      "source": [
        "# HandsOn 2: Finished ðŸ¤–"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kCJwyAaILx8"
      },
      "source": [
        "**How to submit this notebook:** After finishing the HandsOn on google colab, share the uploaded notebook (click share -> Anyone with link) and send the link with the <font color='red'>completed notebook and cell outputs</font> to the course supervisor oliver.hausdoerfer@tum.de. Include your name and student ID in this e-mail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQuusNj8INzl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
